\documentclass[11pt]{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[total={7in,9in}]{geometry}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{hyperref}

\title{Zero knowledge ACTUS for holistic financial data management}
\author{ \\ Morgan Thomas \\ Casper Association \\ morgan@casper.network }


\begin{document}

\maketitle

The ability of financial instutitions to know the risks they are exposed to
depends on their access to high quality data which they can use as input to
models and analyses. To be high quality, the data must be accurate, and also
organized. Accurate but disorganized data is not useful for modeling and analysis
until it is organized, because only well organized data can be used as input to
rigorous, systematic processes which make sense of the data.

Organizing financial data involves collating data from diverse sources.
The problem of collating financial data, which may appear to be a boring problem
at first blush, is not only systemically important but also outrageously
complicated. To get a sense of the scale of the problem, let's consider one
relatively tiny piece of it, which is quite extensive in itself. Let's consider
the problem of presenting the prices of one equity on all of the exchanges
where it is listed, given the price feeds for all of the exchanges. To do this,
we need to be able to map the name or ticker symbol of an equity on one exchange
to the name or ticker symbol of the same equity on any other exchange. The natural
approach is to choose a standard symbology for equities and learn to map all of
the symbologies of each exchange onto the standard symbology. There are dozens
of stock exchanges, and thousands of publicly traded companies, each with one
or more share classes. Furthermore, each exchange regularly lists new stocks, and
the symbology mappings must therefore be updated regularly, ideally in an automated
or mostly automated fashion. In practice, this is not a simple problem.

What would make any collation problem much easier is if the data to be collated
already comes in a homogeneous form. If financial data providers are to arrange
for such a situation, then they require open standards for how financial data is to
be structured.
ACTUS, the Algorithmic Contract Types Unified Standard, takes a bite out of this
problem by offering a standard format for financial contracts. ACTUS takes the
view that basic financial instruments like cash and equities, as well as
debt instruments and derivatives, are all contracts. Contracts are not the only
type we need in an ontology of financial data, but they are certainly a very
useful type which captures a lot of the data types that need to be expressed
in an ontology of financial data. ACTUS itself is not a complete ontology of
contracts, but reportedly (TODO: who says this?) it is rare to find a financial
contract which cannot be expressed within the taxonomy of ACTUS contracts.

Taking ACTUS as a jumping off point, we can begin to imagine a holistic data
management solution for financial institutions. A holistic approach to data
management takes data out of siloes, and puts it into a common database, allowing
connections to be made more easily. For instance, with a holistic data management
approach, the debts and equities of a company can be viewed together in the same
analysis. Let's contrast this with a siloed data management approach.
The debts of a company might be
analyzed by people in a fixed income department, while the equities of the same
company are analyzed by people in an equities department. Those two sets of
people analyzing the same company might not talk to each other or look at each
other's data, and they might not have the technical ability to share data
between each other's systems in a low-friction way. That would be typical of a
siloed data management approach. To some extent, the benefits of a holistic
data management approach are apparent. In this example, we could get a higher
quality analysis of the risk profile of a company by looking at its debts
and its equities together, instead of just its debts or just its equities.

To support the diverse activities of a complex financial institution, such
as an investment bank, a holistic data management solution must support data
types including, but not necessarily limited to, contracts and transactions.
This data should include all contracts the instutition is party to, but also
probably all contracts the instutition is exposed to in some way. For instance,
if a bank owns a mortgage backed security, then ideally their database would
include all of the underlying mortgages and their payment histories, since this
data would be necessary to analyze the risk exposure implied by owning the MBS.
From a risk analysis standpoint, it would be even better if the bank could see
the entire payment histories of all of the borrowers of those mortgages, so they
could estimate the creditworthiness of the borrowers. In practice, this would
present some concerns about privacy.

These considerations suggest that the challenges in designing a holistic financial
data management solution include protecting privacy, and handling potentially
a very large volume of data. Given that each data point is potentially of interest
to multiple institutions, there may be cost savings to be obtained by sharing
data management infrastructure across institutions.\footnote{Sharing data management
infrastructure is already a common practice among financial instutitions, as
I know from my professional experience; however, in my experience, this sharing
is limited to data which is not of a sensitive, private nature.}
Doing so, however, raises concerns not only about privacy but also trust.
A financial institution may not
trust any other institution to manage their data. Also, individuals may have
rights to privacy of their personal data by law, and institutions may wish to
preserve the privacy of their customers to a greater extent than required by law.

Can we have a financial data management solution which does not require trust,
which has sufficient capacity and throughput for the envisioned use cases, and
which protects privacy in a granular access control model? It is our thesis that
we can, and that such a solution could take the shape of a zero knowledge
rollup.

A zero knowledge (ZK) rollup is a blockchain built in a particular way which
supports privacy, scalability, and minimizing trust assumptions. What is meant
by a ``blockchain'' here is simply an append-only ledger of transactions (organized
for convenience into blocks), and a software system for maintaining and interacting
with that ledger. A decentralized system is one which has a sufficiently
high\footnote{what is ``sufficient'' here is left ambiguous} Nakamoto coefficient,
defined as the minimum number of actors in the system which would have to
simultaneously exhibit Byzantine faults in order to cause a Byzantine failure.
In other words, in a decentralized system, there is no small number of actors
who could simultaneously fail or collude to cause the system to deviate from
its intended behavior.

A ZK rollup is a layer two solution, which means that it is built on top of
a pre-existing decentralized blockchain (called a layer one in this context).
The security analysis of a ZK rollup relies on the layer one being decentralized.
If the layer one is decentralized, then no one has to be trusted to ensure its
correct operation. The ZK rollup uses a more centralized architecture, for reasons
of efficiency, but it relies on syncing its state to the layer one in order to
minimize trust assumptions. A layer one blockchain typically achieves decentralization
by using an extensive amount of replication. These duplicated efforts make
it relatively costly to add data to a decentralized layer one blockchain, while
also limiting the rate at which data can be added, due to the communication
overhead involved in coordinating the replicas.

A ZK rollup uses less replication, which allows for improvements in efficiency
/ throughput, while also complicating the security analysis. A ZK rollup
guarantees that the ledger state it records is valid by periodically posting
a hash of the ledger state to the layer one, along with a zero knowledge proof
showing that it is the hash of a valid ledger. If the layer one is programmed
to accept such an update only when the proof is valid, then it is infeasible to
update the layer one to contain an invalid claim about the state of the rollup.
This ensures that all transactions on the ZK rollup are signed by the proper
parties, that all rules of all applicable smart contracts are satisfied, that
no currency is double spent, and so forth.

A ZK rollup may still require some trust assumptions. The zero knowledge proof
protocol allows a user to know, without any trust assumptions, that the rollup
state synced to layer one is valid, or in other words, no invalid transactions
made it in. This still leaves open the possibility that some valid transactions
submitted by users were excluded. If this possibility is left open, then it requires
a trust assumption from rollup users, namely that the rollup infrastructure is
not going to censor them or their counterparties. More sophisticated ZK rollup
designs may eliminate the need for this trust assumption by building in censorship
resistance mechanisms. At that point, a ZK rollup might be fully decentralized,
requiring no trust assumptions from its users.

Zero knowledge proof technology opens up a lot of possibilities when it comes
to protecting privacy and implementing access control models. Data can be recorded
on the rollup as ciphertexts, which can be deciphered only by designated parties,
consistent with an access control model. Validity of the data can be proven,
without revealing the cleartexts, using zero knowledge proofs.

In conclusion, the proposal is that we build a zero knowledge rollup implementing
the ACTUS standard, which can serve as a holistic data management solution for
multiple financial instutitions. Doing so would solve, or at least take a big
bite out of, the systemically important problem of collating financial data
to support risk analysis. By recording data in a homogeneous form and a common
data store, it is possible to reduce the efforts required to perform in depth
risk analysis on complex financial products.

\end{document}
